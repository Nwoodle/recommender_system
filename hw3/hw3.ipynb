{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Renjie Zhu\\Documents\\workspace\\recommender_system\\hw3\n"
     ]
    }
   ],
   "source": [
    "# ms-python.python added\n",
    "import os\n",
    "try:\n",
    "\tos.chdir(os.path.join(os.getcwd(), 'hw3'))\n",
    "\tprint(os.getcwd())\n",
    "except:\n",
    "\tpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Renjie Zhu - A53266114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "\n",
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)\n",
    "\n",
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        yield l.strip().split(',')\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookCount = defaultdict(int)\n",
    "bookSet = set()\n",
    "totalRead = 0\n",
    "user_read = defaultdict(set)\n",
    "data = []\n",
    "\n",
    "for user,book,_ in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    bookCount[book] += 1\n",
    "    bookSet.add(book)\n",
    "    totalRead += 1\n",
    "    user_read[user].add(book)\n",
    "    data.append((user, book))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = data[:190000]\n",
    "validation = data[190000:]\n",
    "validation_new = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user, book in validation:\n",
    "    sample = random.sample(bookSet.difference(user_read[user]), 1)[0]\n",
    "    validation_new.append([user, book, 1])\n",
    "    validation_new.append([user, sample, 0])\n",
    "validation = np.array(validation_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_user = defaultdict(set)\n",
    "user_read = defaultdict(set)\n",
    "\n",
    "for user, book in training:\n",
    "    book_user[book].add(user)\n",
    "    user_read[user].add(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "popular = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    popular.add(i)\n",
    "    if count > totalRead * 0.58: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for _, book, _ in validation:\n",
    "    if book in popular:\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gt = validation[:,2].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.65765.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Baseline accuracy: {sum(pred==valid_gt)/len(pred)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "Better threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.63875; threshold: 40 th\n",
      "Baseline accuracy: 0.64015; threshold: 41 th\n",
      "Baseline accuracy: 0.6419; threshold: 42 th\n",
      "Baseline accuracy: 0.6447; threshold: 43 th\n",
      "Baseline accuracy: 0.6455; threshold: 44 th\n",
      "Baseline accuracy: 0.646; threshold: 45 th\n",
      "Baseline accuracy: 0.6477; threshold: 46 th\n",
      "Baseline accuracy: 0.649; threshold: 47 th\n",
      "Baseline accuracy: 0.6502; threshold: 48 th\n",
      "Baseline accuracy: 0.6521; threshold: 49 th\n",
      "Baseline accuracy: 0.6532; threshold: 50 th\n",
      "Baseline accuracy: 0.65325; threshold: 51 th\n",
      "Baseline accuracy: 0.6549; threshold: 52 th\n",
      "Baseline accuracy: 0.6562; threshold: 53 th\n",
      "Baseline accuracy: 0.65615; threshold: 54 th\n",
      "Baseline accuracy: 0.6561; threshold: 55 th\n",
      "Baseline accuracy: 0.6574; threshold: 56 th\n",
      "Baseline accuracy: 0.6582; threshold: 57 th\n",
      "Baseline accuracy: 0.65765; threshold: 58 th\n",
      "Baseline accuracy: 0.6571; threshold: 59 th\n"
     ]
    }
   ],
   "source": [
    "# better threshold\n",
    "percentile = list(range(40, 60, 1))\n",
    "popularSet = []\n",
    "for p in percentile:\n",
    "    popular = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        popular.add(i)\n",
    "        if count > totalRead * (p/100): break\n",
    "    popularSet.append(popular)\n",
    "\n",
    "    pred = []\n",
    "    for _, book, _ in validation:\n",
    "        if book in popular:\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            pred.append(0)\n",
    "    \n",
    "    print(f\"Baseline accuracy: {sum(pred==valid_gt)/len(pred)}; threshold: {p} th\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The better threshold is 57th percentage from the above test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n",
    "\n",
    "Jaccard similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom\n",
    "\n",
    "def Max_Jaccard(user, book):\n",
    "    similarities = []\n",
    "    b = book_user[book]\n",
    "    for read in user_read[user]:\n",
    "        similarities.append(Jaccard(b, book_user[read]))\n",
    "    \n",
    "    return max(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard accuracy: 0.5996; threshold: 0.005\n",
      "Jaccard accuracy: 0.6064; threshold: 0.006\n",
      "Jaccard accuracy: 0.6124; threshold: 0.006999999999999999\n",
      "Jaccard accuracy: 0.6186; threshold: 0.008\n",
      "Jaccard accuracy: 0.62345; threshold: 0.009\n",
      "Jaccard accuracy: 0.6255; threshold: 0.009999999999999998\n",
      "Jaccard accuracy: 0.62565; threshold: 0.011\n",
      "Jaccard accuracy: 0.62445; threshold: 0.011999999999999999\n",
      "Jaccard accuracy: 0.6232; threshold: 0.012999999999999998\n",
      "Jaccard accuracy: 0.61825; threshold: 0.013999999999999999\n",
      "Jaccard accuracy: 0.61125; threshold: 0.015\n"
     ]
    }
   ],
   "source": [
    "ps = np.linspace(0.005, 0.015, 11)\n",
    "for p in ps:\n",
    "    pred = []\n",
    "    for user, book, _ in validation:\n",
    "        sim = Max_Jaccard(user, book)\n",
    "        if sim > p:\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            pred.append(0)\n",
    "\n",
    "    print(f\"Jaccard accuracy: {sum(pred==valid_gt)/len(pred)}; threshold: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With above test, the best the Jaccard accuracy can do is about a 62.645% accuracy with threshold of 0.011."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\n",
    "\n",
    "Classifier with population and Jaccard accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined accuracy: 0.66425 pop (58), thr (0.004).\n",
      "Combined accuracy: 0.6641 pop (58), thr (0.005).\n",
      "Combined accuracy: 0.66485 pop (58), thr (0.006).\n",
      "Combined accuracy: 0.66465 pop (58), thr (0.007).\n",
      "Combined accuracy: 0.6646 pop (58), thr (0.008).\n",
      "Combined accuracy: 0.6639 pop (58), thr (0.009000000000000001).\n",
      "Combined accuracy: 0.6603 pop (58), thr (0.01).\n",
      "Combined accuracy: 0.6641 pop (59), thr (0.004).\n",
      "Combined accuracy: 0.664 pop (59), thr (0.005).\n",
      "Combined accuracy: 0.66475 pop (59), thr (0.006).\n",
      "Combined accuracy: 0.6647 pop (59), thr (0.007).\n",
      "Combined accuracy: 0.6645 pop (59), thr (0.008).\n",
      "Combined accuracy: 0.66395 pop (59), thr (0.009000000000000001).\n",
      "Combined accuracy: 0.6603 pop (59), thr (0.01).\n",
      "Combined accuracy: 0.66485 pop (60), thr (0.004).\n",
      "Combined accuracy: 0.66495 pop (60), thr (0.005).\n",
      "Combined accuracy: 0.66565 pop (60), thr (0.006).\n",
      "Combined accuracy: 0.6657 pop (60), thr (0.007).\n",
      "Combined accuracy: 0.6656 pop (60), thr (0.008).\n",
      "Combined accuracy: 0.66515 pop (60), thr (0.009000000000000001).\n",
      "Combined accuracy: 0.6617 pop (60), thr (0.01).\n",
      "Combined accuracy: 0.6645 pop (61), thr (0.004).\n",
      "Combined accuracy: 0.66465 pop (61), thr (0.005).\n",
      "Combined accuracy: 0.66545 pop (61), thr (0.006).\n",
      "Combined accuracy: 0.66575 pop (61), thr (0.007).\n",
      "Combined accuracy: 0.66575 pop (61), thr (0.008).\n",
      "Combined accuracy: 0.66545 pop (61), thr (0.009000000000000001).\n",
      "Combined accuracy: 0.66215 pop (61), thr (0.01).\n",
      "Combined accuracy: 0.66435 pop (62), thr (0.004).\n",
      "Combined accuracy: 0.66465 pop (62), thr (0.005).\n",
      "Combined accuracy: 0.66555 pop (62), thr (0.006).\n",
      "Combined accuracy: 0.66595 pop (62), thr (0.007).\n",
      "Combined accuracy: 0.66595 pop (62), thr (0.008).\n",
      "Combined accuracy: 0.66565 pop (62), thr (0.009000000000000001).\n",
      "Combined accuracy: 0.6626 pop (62), thr (0.01).\n",
      "Combined accuracy: 0.66465 pop (63), thr (0.004).\n",
      "Combined accuracy: 0.66495 pop (63), thr (0.005).\n",
      "Combined accuracy: 0.66585 pop (63), thr (0.006).\n",
      "Combined accuracy: 0.6662 pop (63), thr (0.007).\n",
      "Combined accuracy: 0.66625 pop (63), thr (0.008).\n",
      "Combined accuracy: 0.66615 pop (63), thr (0.009000000000000001).\n",
      "Combined accuracy: 0.66305 pop (63), thr (0.01).\n",
      "Combined accuracy: 0.6653 pop (64), thr (0.004).\n",
      "Combined accuracy: 0.6657 pop (64), thr (0.005).\n",
      "Combined accuracy: 0.6669 pop (64), thr (0.006).\n",
      "Combined accuracy: 0.66705 pop (64), thr (0.007).\n",
      "Combined accuracy: 0.66715 pop (64), thr (0.008).\n",
      "Combined accuracy: 0.667 pop (64), thr (0.009000000000000001).\n",
      "Combined accuracy: 0.6639 pop (64), thr (0.01).\n",
      "(0.66715, 64, 0.008)\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for p in range(58,65,1):\n",
    "    for t in np.linspace(0.004,0.010, 7):\n",
    "        popular = set()\n",
    "        count = 0\n",
    "        for ic, i in mostPopular:\n",
    "            count += ic\n",
    "            popular.add(i)\n",
    "            if count > totalRead * (p/100): break\n",
    "\n",
    "        pred = []\n",
    "        for user, book, _ in validation:\n",
    "            sim = Max_Jaccard(user, book)\n",
    "            if (sim > t) and (book in popular):\n",
    "                pred.append(1)\n",
    "            else:\n",
    "                pred.append(0)\n",
    "\n",
    "        print(f\"Combined accuracy: {sum(pred==valid_gt)/len(pred)} pop ({p}), thr ({t}).\")\n",
    "        result.append((sum(pred==valid_gt)/len(pred),p,t))\n",
    "result.sort()\n",
    "print(result[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"predictions_Read.txt\", 'w')\n",
    "popular = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    popular.add(i)\n",
    "    if count > totalRead * (64/100): break\n",
    "\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        f.write(l)\n",
    "        continue\n",
    "    user,book = l.strip().split('-')\n",
    "    sim = Max_Jaccard(user, book)\n",
    "    if (sim > 0.008) and (book in popular):\n",
    "        f.write(user + '-' + book + \",1\\n\")\n",
    "    else:\n",
    "        f.write(user + '-' + book + \",0\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle username is \"Renjie Zhu\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.\n",
    "\n",
    "Rating prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating = []\n",
    "# users = []\n",
    "# items = []\n",
    "for user,book,rating in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    user_rating.append((user, book, int(rating)))\n",
    "    # users.append(user)\n",
    "    # items.append(book)\n",
    "\n",
    "training = user_rating[:190000]\n",
    "validation = user_rating[190000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = sum([int(r) for _,_,r in training]) / len(training)\n",
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)\n",
    "book_user = defaultdict(list)\n",
    "user_book = defaultdict(list)\n",
    "\n",
    "for u,b,r in training:\n",
    "    book_user[u].append((u,b))\n",
    "    user_book[b].append((u,b))\n",
    "\n",
    "\n",
    "nUsers = len(book_user)\n",
    "nItems = len(user_book)\n",
    "users = list(book_user.keys())\n",
    "items = list(user_book.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(user, item):\n",
    "    try:\n",
    "        return alpha + userBiases[user] + itemBiases[item]\n",
    "    except:\n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = dict(zip(users, theta[1:nUsers+1]))\n",
    "    itemBiases = dict(zip(items, theta[1+nUsers:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(u, b) for u,b,_ in training]\n",
    "    cost = MSE(predictions, labels)\n",
    "    # print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for i in itemBiases:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(training)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    for u,b,r in training:\n",
    "        pred = prediction(u, b)\n",
    "        diff = pred - r\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dItemBiases[b] += 2/N*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    return np.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [r for _,_,r in training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.89709313e+00, -9.83268551e-06,  5.26797138e-05, ...,\n",
       "         5.41367671e-07, -8.90170173e-06, -2.46909977e-05]),\n",
       " 1.4734687158357118,\n",
       " {'grad': array([-2.47033492e-08, -3.42983207e-09, -1.04124480e-08, ...,\n",
       "         -2.83062290e-10, -1.05817276e-09, -1.37158955e-09]),\n",
       "  'task': b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'funcalls': 4,\n",
       "  'nit': 2,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),\n",
    "                             derivative, args = (labels, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE on validation set is 1.490780087874981.\n"
     ]
    }
   ],
   "source": [
    "# prediction on validation set\n",
    "pred=[]\n",
    "real=[]\n",
    "for u,b,r in validation:\n",
    "    pred.append(prediction(u,b))\n",
    "    real.append(r)\n",
    "\n",
    "print(f\"The MSE on validation set is {MSE(pred,real)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: largest u92864068 ; smallest u11591742.\n",
      "Book: largest b76915592 ; smallest b57299824.\n"
     ]
    }
   ],
   "source": [
    "print(f\"User: largest {max(userBiases, key=userBiases.get)} ; smallest {min(userBiases, key=userBiases.get)}.\")\n",
    "print(f\"Book: largest {max(itemBiases, key=itemBiases.get)} ; smallest {min(itemBiases, key=itemBiases.get)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE on validation set is 1.1261017419441643 with lambda = 1e-06.\n",
      "The MSE on validation set is 1.1104914253254137 with lambda = 1e-05.\n",
      "The MSE on validation set is 1.1774533947047277 with lambda = 0.0001.\n",
      "The MSE on validation set is 1.3981533564653221 with lambda = 0.001.\n",
      "The MSE on validation set is 1.4785348032993548 with lambda = 0.01.\n"
     ]
    }
   ],
   "source": [
    "lamb = [1e-6,1e-5,1e-4,1e-3,1e-2]\n",
    "for l in lamb:\n",
    "    scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),\n",
    "                             derivative, args = (labels, l))\n",
    "\n",
    "    pred=[]\n",
    "    real=[]\n",
    "    for u,b,r in validation:\n",
    "        pred.append(prediction(u,b))\n",
    "        real.append(r)\n",
    "\n",
    "    print(f\"The MSE on validation set is {MSE(pred,real)} with lambda = {l}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.80834896e+00, -5.11998154e-04,  2.61209893e-01, ...,\n",
       "        -3.35722196e-02, -3.80834971e-01, -7.70535389e-01]),\n",
       " 0.960966738376091,\n",
       " {'grad': array([-9.34277656e-06, -6.08890426e-08, -1.13360640e-07, ...,\n",
       "          2.81785196e-07,  3.77359143e-07,  4.27804254e-07]),\n",
       "  'task': b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'funcalls': 153,\n",
       "  'nit': 121,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),\n",
    "                             derivative, args = (labels, 1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"predictions_Rating.txt\", 'w')\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        f.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split('-')\n",
    "    f.write(u + '-' + b + ',' + str(prediction(u,b)) + '\\n')\n",
    "\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle username is \"Renjie Zhu\"."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
